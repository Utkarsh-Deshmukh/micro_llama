{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "downloads = [\n",
    "    {\n",
    "        \"filename\": \"data/llama3-8b/tokenizer.model\",\n",
    "        \"url\": \"https://huggingface.co/bofenghuang/Meta-Llama-3-8B/resolve/1460c22666392e470910ce3d44ffeb2ab7dbd4df/original/tokenizer.model\",\n",
    "    },\n",
    "    {\n",
    "        \"filename\": \"data/llama3-8b/consolidated.00.pth\",\n",
    "        \"url\": \"https://huggingface.co/bofenghuang/Meta-Llama-3-8B/resolve/1460c22666392e470910ce3d44ffeb2ab7dbd4df/original/consolidated.00.pth\",\n",
    "    },\n",
    "]\n",
    "\n",
    "for download in downloads:\n",
    "    if not os.path.isfile(download[\"filename\"]):\n",
    "        os.makedirs(os.path.dirname(download[\"filename\"]), exist_ok=True)\n",
    "        print(f\"Downloading {download[\"url\"]} to {download[\"filename\"]}\")\n",
    "        urllib.request.urlretrieve(download[\"url\"], download[\"filename\"])\n",
    "    else:\n",
    "        print(f\"File {download[\"filename\"]} already found, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tiktoken tokenizer\n",
    "import torch\n",
    "import micro_llama\n",
    "\n",
    "tokenizer = micro_llama.make_tokenizer(\"data/llama3-8B/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Tiktoken tokenizer\n",
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "prompt_ = tokenizer.decode(tokens)\n",
    "\n",
    "print(prompt)\n",
    "print(tokens)\n",
    "print(prompt_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the RoPE positional embedding\n",
    "N = 64\n",
    "D = 256\n",
    "theta = 500_000\n",
    "theta = 5\n",
    "\n",
    "x = torch.randn(1, D)\n",
    "x = x.expand(N, D) + torch.randn(N, D) * 0.01\n",
    "x = x / x.norm(dim=-1, keepdim=True)\n",
    "\n",
    "y = micro_llama.rope(x.reshape(1, N, 1, D), theta=theta)\n",
    "y = y.reshape(N,D)\n",
    "\n",
    "M = x @ x.transpose(-2, -1)\n",
    "M_ = y @ y.transpose(-2, -1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(M.detach().numpy())\n",
    "plt.title(\"Without RoPE\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"With RoPE\")\n",
    "plt.imshow(M_.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLAMA3 8B model\n",
    "llama = micro_llama.Llama()\n",
    "params = torch.load('data/llama3-8B/consolidated.00.pth', weights_only=True)\n",
    "llama.load_state_dict(params)\n",
    "llama.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the LLAMA3 model\n",
    "prompt = \"the answer to the ultimate question of life, the universe, and everything is \"\n",
    "x = torch.tensor([128000] + tokenizer.encode(prompt))\n",
    "print(tokenizer.decode(list(x)))\n",
    "\n",
    "y = llama(x.unsqueeze(0))\n",
    "print(tokenizer.decode(list(y.argmax(dim=-1)[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
